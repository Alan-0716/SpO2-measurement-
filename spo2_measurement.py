# -*- coding: utf-8 -*-
"""SpO2 measurement.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bFmImTI8OUG-xKO2miVuJ7KDXakM9DVh
"""

from google.colab import files
import matplotlib.pyplot as plt
import numpy as np
import random
import os
 
from keras import utils as np_utils
from keras.models import Sequential
from keras.layers import *

from ipywidgets import interact_manual
import pandas as pd
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from keras import backend as K

uploaded = files.upload()
for fn in uploaded.keys():
    print('User uploaded file "{name}" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))

Seed=55
np.random.seed(Seed)
tf.random.set_seed(Seed)
random.seed(Seed)
os.environ['PYTHONHASHSEED']=str(Seed)

df = pd.read_excel('7道-2.xlsx')
values = df.values
print(values.shape)

all_data_x=values[0:10,7]
all_data_y=values[0:10,0:7]

#以濃度製造數據(x100)
train_x=np.zeros(shape=(1000,1))
train_y=np.zeros(shape=(1000,1000,7))
count = 0
for k in range(0,1000):
  train_x[k,0]=all_data_x[count,] 
  for i in range(0,1000):
    #train_x[0+6*i:6+6*i,0]=all_data_x[0:6,]
    for j in range(0,7):
      train_y[k,i,j]=all_data_y[count,j]*np.random.uniform(0.98,1.02)
  if count > 8:
    count=0
  else:
    count=count+1 
    #train_y[k, i,12]=all_data_y[k,12]

print(train_x)
print(train_y)
print(train_x.shape)
print(train_y.shape)

#打亂數據
indices = np.arange(train_x.shape[0])
np.random.shuffle(indices)

train_x = train_x[indices]
train_y = train_y[indices]
#print(indices)
print(train_x)
#print(train_y[0])

#label = trainxx.keys()
#print(label)
#data = pd.DataFrame(label)
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
trainx=le.fit_transform(train_x)
x_train_hot = tf.keras.utils.to_categorical(trainx)
print(x_train_hot)
print(x_train_hot.shape)

!pip install -q -U git+git://github.com/Hvass-Labs/scikit-optimize.git@dd7433da068b5a2509ef4ea4e5195458393e6555
import skopt
from skopt import gp_minimize, forest_minimize
from skopt.space import Real, Categorical, Integer
from skopt.plots import plot_convergence
from skopt.plots import plot_objective, plot_evaluations
from skopt.plots import plot_histogram, plot_objective_2D
from skopt.utils import use_named_args

def create_model(neurons=50, filters1=10, filters2=20, kernalsize=5, drop_rate=0.15, num_dense_layers=1, lr=1e-3):
  Sequence_length=1000

  model2 = Sequential()
  model2.add(Conv1D(filters1, kernalsize, activation='relu', input_shape=(Sequence_length,7)))
  model2.add(Conv1D(filters1, kernalsize, activation='relu'))
  model2.add(MaxPooling1D(2))
  model2.add(Dropout(drop_rate))
  model2.add(Conv1D(filters2, kernalsize, activation='relu'))
  model2.add(Conv1D(filters2, kernalsize, activation='relu'))
  model2.add(GlobalAveragePooling1D())
  model2.add(Dropout(drop_rate))
  for i in range(num_dense_layers):
    model2.add(Dense(neurons,activation='relu',kernel_initializer='normal',bias_initializer='zeros'))
  model2.add(Dense(10,activation='softmax',kernel_initializer='normal',bias_initializer='zeros'))
  optimizer=Adam(lr=lr)
  model2.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])
  return model2

batch_size = Integer(low=10, high=300,name='batch_size')
epochs = Integer(low=10, high=500,name='epochs')
num_dense_layers = Integer(low=1, high=10,name='num_dense_layers')
filters1 = Integer(low=2, high=64,name='filters1')
filters2 = Integer(low=2, high=64,name='filters2')
kernalsize = Integer(low=2, high=32, name='kernalsize')
neurons = Integer(low=10, high=100,name='neurons')
drop_rate = Real(0.15, 0.2, name='drop_rate')
lr = Real(1e-4, 1e-1, name='lr')
param_grid = [neurons, filters1, filters2, kernalsize, drop_rate, num_dense_layers, lr, batch_size, epochs]

@use_named_args(dimensions=param_grid)
def fitless(neurons, filters1, filters2, kernalsize, drop_rate, num_dense_layers, lr, batch_size, epochs):

    # Create the model using a specified hyperparameters.
    model = create_model(neurons, filters1, filters2, [kernalsize], drop_rate, num_dense_layers)
    print('neuron: ',neurons)
    print('filters1: ',filters1)
    print('filters2: ',filters2)
    print('kernalsize: ',kernalsize)
    print('drop_rate: ',drop_rate)
    print('num_dense_layers: ',num_dense_layers)
    print('learning_rate: ',filters1)
    print('batch_size: ',batch_size)
    print('epochs: ',epochs)
    print()
    # Train the model with the train dataset.
    model.fit(train_y, x_train_hot, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)
    # Evaluate the model with the eval dataset.
    score = model.evaluate(train_y, x_train_hot, steps=10, verbose=2)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])

    # Return the accuracy.

    return -score[1]

optimizer = gp_minimize(func=fitless, dimensions=param_grid, acq_func='EI', n_calls=18, n_jobs=-1)

plot_convergence(optimizer)

optimizer.x

space = optimizer.space
space.point_to_dict(optimizer.x)

sorted(zip(optimizer.func_vals, optimizer.x_iters))

dim_names = ['neurons', 'filters', 'drop_rate', 'num_dense_layers', 'batch_size', 'epochs']
fig, ax = plot_objective(result=optimizer, dimension_names=dim_names)

Sequence_length=1000

model2 = Sequential()
model2.add(Conv1D(10, 5, activation='relu', input_shape=(Sequence_length,7)))
model2.add(Conv1D(10, 5,activation='relu'))
model2.add(MaxPooling1D(2))
#model2.add(GlobalAveragePooling1D())
#model2.add(Dropout(0.17))
model2.add(Conv1D(20, 5, activation='relu'))
model2.add(Conv1D(20, 5,  activation='relu'))
model2.add(GlobalAveragePooling1D())

#model2.add(Flatten())
model2.add(Dense(50,activation='relu',kernel_initializer='normal',bias_initializer='zeros'))
model2.add(Dropout(0.4))
model2.add(Dense(10,activation='softmax',kernel_initializer='normal',bias_initializer='zeros'))

model2.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])
print(model2.summary())

history = model2.fit(train_y, x_train_hot, epochs=100, batch_size=10, validation_split=0.2, verbose=2)

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')

plt.legend(['train', 'validation'], loc='upper right')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.ylim(0,1)

plt.legend(['train', 'validation'], loc='upper left')
plt.show()

_,accuracy =model2.evaluate(train_y, x_train_hot, verbose=2)

from sklearn.model_selection import GridSearchCV

def create_modelg(neurons=50, filters1=10, filters2=20, kernalsize=5, drop_rate=0.15, num_dense_layers=1, lr=1e-3):
  Sequence_length=1000

  model2 = Sequential()
  model2.add(Conv1D(filters1, kernalsize, activation='relu', input_shape=(Sequence_length,7)))
  model2.add(Conv1D(filters1, kernalsize, activation='relu'))
  model2.add(MaxPooling1D(2))
  model2.add(Dropout(drop_rate))
  model2.add(Conv1D(filters2, kernalsize, activation='relu'))
  model2.add(Conv1D(filters2, kernalsize, activation='relu'))
  model2.add(GlobalAveragePooling1D())
  model2.add(Dropout(drop_rate))
  for i in range(num_dense_layers):
    model2.add(Dense(neurons,activation='relu',kernel_initializer='normal',bias_initializer='zeros'))
  model2.add(Dense(10,activation='softmax',kernel_initializer='normal',bias_initializer='zeros'))
  optimizer=Adam(lr=lr)
  model2.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])
  return model2

param_grid = [
{'batch_size': [10, 50, 100, 200, 300],
 'drop_rate': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],
 'epochs': [10, 50, 100, 200, 300, 400, 500],
 'filters1': [2, 4, 8, 16 ,32 ,64],
 'filters2': [2, 4, 8, 16 ,32 ,64],
 'kernalsize': [2, 4, 8, 16 ,32],
 'lr': [0.0001, 0.001, 0.01,0.1],
 'neurons': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
 'num_dense_layers': [2, 4, 6, 8, 10]},
]
model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_modelg, epochs=5, verbose=1)

grid_search = GridSearchCV(model, param_grid, cv=5)
 
grid_search.fit(train_y, x_train_hot, epochs=100, batch_size=10, validation_split=0.2, verbose=2)

print(f"最佳準確率: {grid_result.best_score_}，最佳參數組合：{grid_result.best_params_}")
